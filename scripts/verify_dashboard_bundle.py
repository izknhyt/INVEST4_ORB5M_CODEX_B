"""Verify dashboard export bundles before distribution.

This CLI validates the latest dashboard export manifest, re-computes
dataset checksums, and enforces retention expectations for the history
archive.  The implementation mirrors the Phase 3 detailed design so the
automation pipeline can gate uploads on a single deterministic command.
"""

from __future__ import annotations

import argparse
import hashlib
import json
import sys
import time
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Mapping, MutableMapping, Optional, Sequence, Tuple

if __package__ in (None, ""):
    sys.path.append(str(Path(__file__).resolve().parents[1]))

from scripts._automation_context import build_automation_context
from scripts._automation_logging import (
    AUTOMATION_LOG_PATH,
    AUTOMATION_SCHEMA_PATH,
    AUTOMATION_SEQUENCE_PATH,
    AutomationLogError,
    AutomationLogSchemaError,
    log_automation_event_with_sequence,
)
from scripts._schema import SchemaValidationError, load_json_schema, validate_json_schema


DEFAULT_JOB_NAME = "dashboard-bundle-verification"
DEFAULT_MANIFEST_PATH = Path("out/dashboard/manifest.json")
DEFAULT_HISTORY_DIR = Path("ops/dashboard_export_history")
DEFAULT_ARCHIVE_MANIFEST = Path("ops/dashboard_export_archive_manifest.jsonl")
DEFAULT_RETENTION_DAYS = 56
DEFAULT_EXPECTED_DATASETS: Tuple[str, ...] = ("ev_history", "slippage", "turnover", "latency")


@dataclass(slots=True)
class DatasetStatus:
    """Represents verification details for a single dataset entry."""

    name: str
    status: str
    path: Path
    checksum: Optional[str]
    expected_checksum: Optional[str]
    row_count: Optional[int]
    expected_rows: Optional[int]
    messages: List[str]

    def as_dict(self) -> Dict[str, Any]:
        payload: Dict[str, Any] = {
            "dataset": self.name,
            "status": self.status,
            "path": str(self.path),
        }
        if self.checksum is not None:
            payload["checksum_sha256"] = self.checksum
        if self.expected_checksum is not None:
            payload["expected_checksum_sha256"] = self.expected_checksum
        if self.row_count is not None:
            payload["row_count"] = self.row_count
        if self.expected_rows is not None:
            payload["expected_row_count"] = self.expected_rows
        if self.messages:
            payload["messages"] = list(self.messages)
        return payload


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Validate dashboard export artefacts")
    parser.add_argument("--job-name", default=DEFAULT_JOB_NAME, help="Automation job name for logging context")
    parser.add_argument("--job-id", help="Explicit job identifier (defaults to derived value from --job-name)")
    parser.add_argument(
        "--manifest",
        type=Path,
        default=DEFAULT_MANIFEST_PATH,
        help="Path to the dashboard manifest JSON generated by export_dashboard_data.py",
    )
    parser.add_argument(
        "--expected-dataset",
        action="append",
        dest="expected_datasets",
        default=None,
        help="Dataset expected in the manifest (ev_history, slippage, turnover, latency)."
        " May be specified multiple times.",
    )
    parser.add_argument(
        "--history-dir",
        type=Path,
        default=DEFAULT_HISTORY_DIR,
        help="History directory populated by export_dashboard_data.py",
    )
    parser.add_argument(
        "--archive-manifest",
        type=Path,
        default=DEFAULT_ARCHIVE_MANIFEST,
        help="Archive manifest JSONL tracking pruned history exports",
    )
    parser.add_argument(
        "--retention-days",
        type=int,
        default=DEFAULT_RETENTION_DAYS,
        help="Retention window (in days) enforced for history directories",
    )
    parser.add_argument(
        "--reference-time",
        type=str,
        default=None,
        help="Override current timestamp for retention checks (ISO-8601 UTC)",
    )
    parser.add_argument("--json-out", type=Path, help="Optional JSON file to persist the verification summary")
    parser.add_argument("--indent", type=int, default=2, help="Indentation used when writing --json-out (default: 2)")
    return parser.parse_args(argv)


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)
    start_time = time.perf_counter()

    context = build_automation_context(
        args.job_name,
        job_id=args.job_id,
        argv=["python3", "scripts/verify_dashboard_bundle.py", *(_coerce_argv(argv) if argv is not None else sys.argv[1:])],
    )

    expected_datasets = tuple(args.expected_datasets) if args.expected_datasets else DEFAULT_EXPECTED_DATASETS
    manifest_path = args.manifest.resolve()
    history_dir = args.history_dir.resolve() if args.history_dir else None
    archive_manifest = args.archive_manifest.resolve() if args.archive_manifest else None
    reference_time = _parse_reference_time(args.reference_time)

    artefacts: List[str] = []
    errors: List[Mapping[str, Any]] = []
    dataset_status: Dict[str, DatasetStatus] = {}
    history_summary: Optional[Dict[str, Any]] = None
    archive_summary: Optional[Dict[str, Any]] = None
    manifest_summary: Optional[Dict[str, Any]] = None

    try:
        manifest = _load_manifest(manifest_path)
        artefacts.append(str(manifest_path))
        manifest_summary = {
            "sequence": manifest.get("sequence"),
            "generated_at": manifest.get("generated_at"),
            "job_id": manifest.get("job_id"),
            "dataset_count": len(manifest.get("datasets", [])),
        }
    except VerificationFailure as exc:
        errors.append(exc.as_dict())
        manifest = None

    if manifest is not None:
        dataset_status, dataset_errors, dataset_artefacts = _verify_datasets(
            manifest, manifest_path, expected_datasets
        )
        artefacts.extend(dataset_artefacts)
        errors.extend(dataset_errors)

    if history_dir is not None:
        history_summary, history_errors = _verify_history(history_dir, args.retention_days, reference_time)
        errors.extend(history_errors)
        if history_dir.exists():
            artefacts.append(str(history_dir))

    if archive_manifest is not None:
        archive_summary, archive_errors = _verify_archive_manifest(archive_manifest)
        errors.extend(archive_errors)
        if archive_manifest.exists():
            artefacts.append(str(archive_manifest))

    duration_ms = int((time.perf_counter() - start_time) * 1000)
    status = "ok" if not errors else "error"

    summary: Dict[str, Any] = {
        "status": status,
        "job_id": context.job_id,
        "duration_ms": duration_ms,
    }
    if manifest_summary:
        summary["manifest"] = manifest_summary
    if dataset_status:
        summary["datasets"] = {name: item.as_dict() for name, item in dataset_status.items()}
    if history_summary:
        summary["history"] = history_summary
    if archive_summary:
        summary["archive"] = archive_summary
    if errors:
        summary["errors"] = [dict(item) for item in errors]

    print(json.dumps(summary, ensure_ascii=False))
    _write_optional_json(args.json_out, summary, indent=args.indent)

    diagnostics: Dict[str, Any] = {
        "context": context.as_log_payload(),
    }
    if errors:
        diagnostics["errors"] = [dict(item) for item in errors]

    try:
        log_automation_event_with_sequence(
            context.job_id,
            status,
            log_path=AUTOMATION_LOG_PATH,
            sequence_path=AUTOMATION_SEQUENCE_PATH,
            schema_path=AUTOMATION_SCHEMA_PATH,
            duration_ms=duration_ms,
            attempts=1,
            artefacts=artefacts,
            diagnostics=diagnostics,
        )
    except (AutomationLogError, AutomationLogSchemaError) as exc:
        summary.setdefault("logging_error", str(exc))

    return 0 if status == "ok" else 1


class VerificationFailure(RuntimeError):
    """Wrapper for fatal verification errors."""

    def __init__(self, code: str, message: str, *, details: Optional[Mapping[str, Any]] = None) -> None:
        super().__init__(message)
        self.code = code
        self.message = message
        self.details = dict(details or {})

    def as_dict(self) -> Dict[str, Any]:
        payload = {"code": self.code, "message": self.message}
        if self.details:
            payload["details"] = dict(self.details)
        return payload


def _coerce_argv(argv: Optional[Sequence[str]]) -> Sequence[str]:
    if argv is None:
        return sys.argv[1:]
    return list(argv)


def _parse_reference_time(raw: Optional[str]) -> datetime:
    if not raw:
        return datetime.now(timezone.utc)
    try:
        parsed = datetime.fromisoformat(raw.replace("Z", "+00:00"))
    except ValueError as exc:  # pragma: no cover - guarded via CLI validation
        raise VerificationFailure("invalid_reference_time", f"Invalid reference time: {raw}") from exc
    if parsed.tzinfo is None:
        parsed = parsed.replace(tzinfo=timezone.utc)
    return parsed.astimezone(timezone.utc)


def _load_manifest(path: Path) -> Mapping[str, Any]:
    if not path.exists():
        raise VerificationFailure("manifest_missing", f"Manifest not found: {path}")
    try:
        payload = json.loads(path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        raise VerificationFailure("manifest_invalid_json", f"Manifest is not valid JSON: {path}") from exc

    schema = load_json_schema(Path("schemas/dashboard_manifest.schema.json"))
    try:
        validate_json_schema(payload, schema)
    except SchemaValidationError as exc:
        raise VerificationFailure("manifest_schema_error", str(exc)) from exc
    return payload


def _verify_datasets(
    manifest: Mapping[str, Any],
    manifest_path: Path,
    expected_datasets: Sequence[str],
) -> Tuple[Dict[str, DatasetStatus], List[Mapping[str, Any]], List[str]]:
    dataset_entries = manifest.get("datasets") or []
    entry_map: Dict[str, Mapping[str, Any]] = {}
    for entry in dataset_entries:
        name = entry.get("dataset")
        if isinstance(name, str):
            entry_map[name] = entry

    statuses: Dict[str, DatasetStatus] = {}
    errors: List[Mapping[str, Any]] = []
    artefacts: List[str] = []

    expected_set = {dataset for dataset in expected_datasets}
    for dataset in expected_set:
        entry = entry_map.get(dataset)
        if entry is None:
            status = DatasetStatus(
                dataset,
                "error",
                manifest_path.parent / f"{dataset}.json",
                checksum=None,
                expected_checksum=None,
                row_count=None,
                expected_rows=None,
                messages=["Dataset missing from manifest"],
            )
            statuses[dataset] = status
            errors.append({"code": "dataset_missing", "dataset": dataset})
            continue

        status, entry_errors = _verify_dataset_entry(dataset, entry, manifest, manifest_path)
        statuses[dataset] = status
        if status.path.exists():
            artefacts.append(str(status.path))
        errors.extend(entry_errors)

    # surface unexpected datasets for completeness
    unexpected = sorted(set(entry_map) - expected_set)
    for dataset in unexpected:
        entry = entry_map[dataset]
        checksum_value = entry.get("checksum_sha256") if isinstance(entry.get("checksum_sha256"), str) else None
        row_value = entry.get("row_count") if isinstance(entry.get("row_count"), int) else None
        status = DatasetStatus(
            dataset,
            "warning",
            _resolve_dataset_path(entry.get("path"), manifest_path.parent),
            checksum=checksum_value,
            expected_checksum=checksum_value,
            row_count=row_value,
            expected_rows=row_value,
            messages=["Dataset present in manifest but not listed in expected set"],
        )
        statuses.setdefault(dataset, status)

    return statuses, errors, artefacts


def _verify_dataset_entry(
    dataset: str,
    entry: Mapping[str, Any],
    manifest: Mapping[str, Any],
    manifest_path: Path,
) -> Tuple[DatasetStatus, List[Mapping[str, Any]]]:
    messages: List[str] = []
    errors: List[Mapping[str, Any]] = []
    dataset_path = _resolve_dataset_path(entry.get("path"), manifest_path.parent)

    checksum = None
    try:
        payload, payload_error, checksum = _read_dataset_payload(dataset_path)
    except FileNotFoundError:
        messages.append("Dataset file not found")
        errors.append({"code": "dataset_file_missing", "dataset": dataset, "path": str(dataset_path)})
        status = DatasetStatus(dataset, "error", dataset_path, None, entry.get("checksum_sha256"), None, entry.get("row_count"), messages)
        return status, errors

    if payload_error:
        messages.append(payload_error)
        errors.append(
            {"code": "dataset_json_error", "dataset": dataset, "path": str(dataset_path), "message": payload_error}
        )

    expected_checksum = entry.get("checksum_sha256")
    if expected_checksum and checksum.lower() != str(expected_checksum).lower():
        messages.append("Checksum mismatch")
        errors.append(
            {
                "code": "checksum_mismatch",
                "dataset": dataset,
                "expected": str(expected_checksum),
                "actual": checksum,
            }
        )

    expected_rows = entry.get("row_count") if isinstance(entry.get("row_count"), int) else None
    row_count = None
    if payload is not None:
        row_count, row_messages = _extract_row_count(dataset, payload)
        messages.extend(row_messages)
        for msg in row_messages:
            errors.append({"code": "row_count_mismatch", "dataset": dataset, "message": msg})
        dataset_field = payload.get("dataset")
        if dataset_field is not None and dataset_field != dataset:
            mismatch = f"Dataset field mismatch (expected {dataset}, got {dataset_field})"
            messages.append(mismatch)
            errors.append({"code": "dataset_name_mismatch", "dataset": dataset, "message": mismatch})
        source_hash_error = _verify_source_hash(dataset, payload, entry)
        if source_hash_error:
            messages.append(source_hash_error)
            errors.append(
                {
                    "code": "source_hash_mismatch",
                    "dataset": dataset,
                    "message": source_hash_error,
                }
            )
        job_id_error = _verify_job_id(manifest, payload)
        if job_id_error:
            messages.append(job_id_error)
            errors.append({"code": "job_mismatch", "dataset": dataset, "message": job_id_error})
        generated_at_error = _verify_generated_at(entry, payload)
        if generated_at_error:
            messages.append(generated_at_error)
            errors.append({"code": "generated_at_mismatch", "dataset": dataset, "message": generated_at_error})

    if expected_rows is not None and row_count is not None and row_count != expected_rows:
        mismatch = f"Row count mismatch (manifest {expected_rows}, computed {row_count})"
        messages.append(mismatch)
        errors.append({"code": "row_count_mismatch", "dataset": dataset, "message": mismatch})

    status = "ok" if not messages else "error"
    dataset_status = DatasetStatus(
        dataset,
        status,
        dataset_path,
        checksum,
        str(expected_checksum) if expected_checksum is not None else None,
        row_count,
        expected_rows,
        messages,
    )
    return dataset_status, errors


def _extract_row_count(dataset: str, payload: Mapping[str, Any]) -> Tuple[Optional[int], List[str]]:
    errors: List[str] = []
    if dataset == "ev_history":
        rows = payload.get("rows")
        if isinstance(rows, list):
            count = len(rows)
        else:
            count = None
            errors.append("ev_history rows missing or not a list")
    elif dataset == "slippage":
        state_rows = payload.get("state")
        exec_rows = payload.get("execution")
        if isinstance(state_rows, list) and isinstance(exec_rows, list):
            count = len(state_rows) + len(exec_rows)
        else:
            count = None
            errors.append("slippage state/execution lists missing")
    elif dataset == "turnover":
        rows = payload.get("rows")
        if isinstance(rows, list):
            count = len(rows)
        else:
            count = None
            errors.append("turnover rows missing or not a list")
    elif dataset == "latency":
        rows = payload.get("rows")
        if isinstance(rows, list):
            count = len(rows)
        else:
            count = None
            errors.append("latency rows missing or not a list")
    else:
        count = None
        errors.append(f"Unknown dataset '{dataset}'")
    expected = payload.get("row_count") if isinstance(payload.get("row_count"), int) else None
    if expected is not None and count is not None and count != expected:
        errors.append(f"Row count {count} does not match dataset payload row_count {expected}")
    return count, errors


def _verify_source_hash(dataset: str, payload: Mapping[str, Any], entry: Mapping[str, Any]) -> Optional[str]:
    sources = payload.get("sources")
    if not isinstance(sources, Mapping):
        return "Dataset sources missing or not a mapping"
    expected_hash = entry.get("source_hash")
    if not isinstance(expected_hash, str):
        return "Manifest entry missing source_hash"
    actual = _hash_sources(sources)
    if actual.lower() != expected_hash.lower():
        return f"Source hash mismatch (expected {expected_hash}, got {actual})"
    return None


def _verify_job_id(manifest: Mapping[str, Any], payload: Mapping[str, Any]) -> Optional[str]:
    manifest_job = manifest.get("job_id")
    dataset_job = payload.get("job_id")
    if manifest_job is None or dataset_job is None:
        return "job_id missing in manifest or dataset payload"
    if manifest_job != dataset_job:
        return f"job_id mismatch (manifest {manifest_job}, dataset {dataset_job})"
    return None


def _verify_generated_at(entry: Mapping[str, Any], payload: Mapping[str, Any]) -> Optional[str]:
    manifest_generated = entry.get("generated_at")
    dataset_generated = payload.get("generated_at")
    if manifest_generated is None or dataset_generated is None:
        return "generated_at missing in manifest entry or dataset payload"
    if manifest_generated != dataset_generated:
        return f"generated_at mismatch (manifest {manifest_generated}, dataset {dataset_generated})"
    return None


def _verify_history(
    history_dir: Path,
    retention_days: int,
    reference_time: datetime,
) -> Tuple[Dict[str, Any], List[Mapping[str, Any]]]:
    summary: Dict[str, Any] = {"path": str(history_dir), "directories": []}
    errors: List[Mapping[str, Any]] = []

    if not history_dir.exists():
        errors.append({"code": "history_missing", "path": str(history_dir)})
        return summary, errors

    threshold = None
    if retention_days is not None and retention_days >= 0:
        threshold = reference_time - timedelta(days=retention_days)

    for subdir in sorted(history_dir.iterdir()):
        if not subdir.is_dir():
            continue
        record: MutableMapping[str, Any] = {"name": subdir.name}
        job_ts = _parse_job_timestamp(subdir.name)
        if job_ts is None:
            record["status"] = "invalid"
            errors.append({"code": "history_invalid_name", "directory": subdir.name})
        else:
            record["timestamp"] = job_ts.isoformat().replace("+00:00", "Z")
            if threshold and job_ts < threshold:
                record["status"] = "stale"
                errors.append({"code": "history_retention_violation", "directory": subdir.name})
            else:
                record["status"] = "ok"
        summary["directories"].append(record)
    return summary, errors


def _verify_archive_manifest(path: Path) -> Tuple[Dict[str, Any], List[Mapping[str, Any]]]:
    summary: Dict[str, Any] = {"path": str(path), "entries": 0}
    errors: List[Mapping[str, Any]] = []
    if not path.exists():
        return summary, errors

    lines = path.read_text(encoding="utf-8").splitlines()
    summary["entries"] = len(lines)
    for index, line in enumerate(lines, start=1):
        if not line.strip():
            continue
        try:
            payload = json.loads(line)
        except json.JSONDecodeError:
            errors.append({"code": "archive_invalid_json", "line": index})
            continue
        if not isinstance(payload, Mapping) or "job_id" not in payload:
            errors.append({"code": "archive_invalid_entry", "line": index})
            continue
    return summary, errors


def _resolve_dataset_path(path_value: Any, base_dir: Path) -> Path:
    path = Path(str(path_value)) if path_value else base_dir
    if not path.is_absolute():
        path = (base_dir / path).resolve()
    return path


def _hash_sources(sources: Mapping[str, Any]) -> str:
    canonical = json.dumps(dict(sorted(sources.items())), separators=(",", ":"))
    return hashlib.sha256(canonical.encode("utf-8")).hexdigest()


def _read_dataset_payload(path: Path) -> Tuple[Optional[Mapping[str, Any]], Optional[str], Optional[str]]:
    text = path.read_text(encoding="utf-8")
    try:
        data = json.loads(text)
    except json.JSONDecodeError:
        checksum = hashlib.sha256(text.encode("utf-8")).hexdigest()
        return None, "Dataset payload is not valid JSON", checksum
    if not isinstance(data, Mapping):
        checksum = hashlib.sha256(text.encode("utf-8")).hexdigest()
        return None, "Dataset payload must be a JSON object", checksum
    canonical = json.dumps(data, ensure_ascii=False, separators=(",", ":"))
    checksum = hashlib.sha256(canonical.encode("utf-8")).hexdigest()
    return data, None, checksum


def _write_optional_json(target: Optional[Path], payload: Mapping[str, Any], *, indent: int) -> None:
    if not target:
        return
    target.parent.mkdir(parents=True, exist_ok=True)
    target.write_text(json.dumps(payload, ensure_ascii=False, indent=indent) + "\n", encoding="utf-8")


def _parse_job_timestamp(job_id: str) -> Optional[datetime]:
    prefix = job_id.split("-", 1)[0]
    if not prefix.endswith("Z"):
        return None
    raw = prefix[:-1]
    try:
        parsed = datetime.strptime(raw, "%Y%m%dT%H%M%S")
    except ValueError:
        return None
    return parsed.replace(tzinfo=timezone.utc)


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    sys.exit(main())
